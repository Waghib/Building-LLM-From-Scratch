{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cE5c-lrWaLEy",
        "outputId": "fc689533-0c72-475c-b063-e2f0b322cd45"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "EijH4TFgHBzi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_probs, V)\n",
        "        return output\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        batch_size, seq_length, d_model = x.size()\n",
        "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        batch_size, _, seq_length, d_k = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        Q = self.split_heads(self.W_q(Q))\n",
        "        K = self.split_heads(self.W_k(K))\n",
        "        V = self.split_heads(self.W_v(V))\n",
        "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "        output = self.W_o(self.combine_heads(attn_output))\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "DgYZxikiHKNt"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))\n"
      ],
      "metadata": {
        "id": "zuFjvHVyHQyg"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "FiqvhwpEHSjf"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
        "        x = self.norm2(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm3(x + self.dropout(ff_output))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "ePcFe74cOreS"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_mask(self, src, tgt):\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
        "        seq_length = tgt.size(1)\n",
        "        nopeak_mask = torch.tril(torch.ones((seq_length, seq_length), device=tgt.device)).unsqueeze(0).bool()\n",
        "        tgt_mask = tgt_mask & nopeak_mask\n",
        "        return src_mask, tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
        "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
        "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
        "\n",
        "        enc_output = src_embedded\n",
        "        for enc_layer in self.encoder_layers:\n",
        "            enc_output = enc_layer(enc_output, src_mask)\n",
        "\n",
        "        dec_output = tgt_embedded\n",
        "        for dec_layer in self.decoder_layers:\n",
        "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
        "\n",
        "        output = self.fc(dec_output)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "2R42eEOiHUv8"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/drive/MyDrive/samsum-train.csv\")\n",
        "dialogues = [str(d) for d in data['dialogue'].tolist()]\n",
        "summaries = [str(s) for s in data['summary'].tolist()]\n"
      ],
      "metadata": {
        "id": "loJlOyJFHWuV"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "MAX_SRC_LEN = 128\n",
        "MAX_TGT_LEN = 64\n",
        "\n",
        "def tokenize_and_pad(texts, max_length, tokenizer):\n",
        "    return tokenizer(\n",
        "        texts,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "input_encodings = tokenize_and_pad(dialogues, MAX_SRC_LEN, tokenizer)\n",
        "target_encodings = tokenize_and_pad(summaries, MAX_TGT_LEN, tokenizer)\n"
      ],
      "metadata": {
        "id": "0dvFjpRvLY7Z"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_inputs, val_inputs, train_labels, val_labels = train_test_split(\n",
        "    input_encodings[\"input_ids\"], target_encodings[\"input_ids\"], test_size=0.1\n",
        ")\n",
        "\n",
        "class TextSummarizationDataset(Dataset):\n",
        "    def __init__(self, inputs, labels):\n",
        "        self.inputs = inputs\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input': self.inputs[idx],\n",
        "            'label': self.labels[idx]\n",
        "        }\n",
        "\n",
        "train_dataset = TextSummarizationDataset(train_inputs, train_labels)\n",
        "val_dataset = TextSummarizationDataset(val_inputs, val_labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64)\n"
      ],
      "metadata": {
        "id": "kfd9iPHYMCwu"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "src_vocab_size = tokenizer.vocab_size\n",
        "tgt_vocab_size = tokenizer.vocab_size\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "d_ff = 2048\n",
        "max_seq_length = 4096\n",
        "dropout = 0.1\n",
        "\n",
        "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n"
      ],
      "metadata": {
        "id": "KK-MMVFSMGRu"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
        "    model.to(device)\n",
        "    best_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    patience_threshold = 3\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            src = batch['input'].to(device)\n",
        "            tgt = batch['label'].to(device)\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            tgt_output = tgt[:, 1:]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(src, tgt_input)\n",
        "            predictions = predictions.reshape(-1, predictions.size(-1))\n",
        "            tgt_output = tgt_output.reshape(-1)\n",
        "            loss = criterion(predictions, tgt_output)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                src = batch['input'].to(device)\n",
        "                tgt = batch['label'].to(device)\n",
        "                tgt_input = tgt[:, :-1]\n",
        "                tgt_output = tgt[:, 1:]\n",
        "\n",
        "                predictions = model(src, tgt_input)\n",
        "                predictions = predictions.reshape(-1, predictions.size(-1))\n",
        "                tgt_output = tgt_output.reshape(-1)\n",
        "                loss = criterion(predictions, tgt_output)\n",
        "\n",
        "                total_val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        if avg_val_loss < best_loss:\n",
        "          best_loss = avg_val_loss\n",
        "          torch.save(model.state_dict(), \"best_transformer_model.pth\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience_threshold:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "\n",
        "    print(\"Training complete.\")"
      ],
      "metadata": {
        "id": "whi3a6NHMNqQ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(model, src, tokenizer, max_len, device):\n",
        "    model.eval()\n",
        "    src = src.to(device)\n",
        "    src_mask = (src != tokenizer.pad_token_id).unsqueeze(1).unsqueeze(2)\n",
        "    src_embedded = model.dropout(model.positional_encoding(model.encoder_embedding(src)))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        enc_output = src_embedded\n",
        "        for enc_layer in model.encoder_layers:\n",
        "            enc_output = enc_layer(enc_output, src_mask)\n",
        "\n",
        "    tgt_tokens = torch.tensor([[tokenizer.cls_token_id]], device=device)\n",
        "    for _ in range(max_len):\n",
        "        tgt_mask = torch.tril(torch.ones((tgt_tokens.size(1), tgt_tokens.size(1)), device=device)).bool().unsqueeze(0)\n",
        "        tgt_embedded = model.dropout(model.positional_encoding(model.decoder_embedding(tgt_tokens)))\n",
        "        dec_output = tgt_embedded\n",
        "\n",
        "        for dec_layer in model.decoder_layers:\n",
        "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
        "\n",
        "        predictions = model.fc(dec_output[:, -1, :])\n",
        "        next_token = predictions.argmax(dim=-1).unsqueeze(0)\n",
        "\n",
        "\n",
        "        if next_token.item() == tokenizer.sep_token_id:\n",
        "            break\n",
        "\n",
        "        tgt_tokens = torch.cat([tgt_tokens, next_token], dim=1)\n",
        "\n",
        "    return tokenizer.decode(tgt_tokens.squeeze().tolist(), skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "sffryTnJTtxk"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ngx4LIAATujJ",
        "outputId": "d693f088-1d76-4e41-cf42-8e4c57c4c351"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.66.6)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=68691d78a8d2b13554165ef40fae46846ea0561bda12456209e31fe34efe983f\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "def evaluate_model(model, data_loader, tokenizer, max_len, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            src = batch['input'].to(device)\n",
        "            tgt = batch['label'].to(device)\n",
        "            for i in range(src.size(0)):\n",
        "                src_sentence = tokenizer.decode(src[i].tolist(), skip_special_tokens=True)\n",
        "                tgt_sentence = tokenizer.decode(tgt[i].tolist(), skip_special_tokens=True)\n",
        "                generated_summary = generate_summary(model, src[i].unsqueeze(0), tokenizer, max_len, device)\n",
        "                scores = scorer.score(tgt_sentence, generated_summary)\n",
        "                for key in rouge_scores:\n",
        "                    rouge_scores[key].append(scores[key].fmeasure)\n",
        "\n",
        "    avg_scores = {key: sum(values) / len(values) for key, values in rouge_scores.items()}\n",
        "    return avg_scores\n"
      ],
      "metadata": {
        "id": "H8ww2oO0Ty7Y"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(\n",
        "    model, train_loader, val_loader, criterion, optimizer,\n",
        "    num_epochs=60, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gX6kACQT2bh",
        "outputId": "929627f4-d3fd-4791-f92b-0a50466173f4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60, Train Loss: 6.2285, Val Loss: 5.3040\n",
            "Epoch 2/60, Train Loss: 5.1003, Val Loss: 4.9770\n",
            "Epoch 3/60, Train Loss: 4.7997, Val Loss: 4.7986\n",
            "Epoch 4/60, Train Loss: 4.5873, Val Loss: 4.6843\n",
            "Epoch 5/60, Train Loss: 4.3777, Val Loss: 4.5007\n",
            "Epoch 6/60, Train Loss: 4.1128, Val Loss: 4.3349\n",
            "Epoch 7/60, Train Loss: 3.8636, Val Loss: 4.2128\n",
            "Epoch 8/60, Train Loss: 3.6263, Val Loss: 4.1178\n",
            "Epoch 9/60, Train Loss: 3.4092, Val Loss: 4.0555\n",
            "Epoch 10/60, Train Loss: 3.2043, Val Loss: 4.0066\n",
            "Epoch 11/60, Train Loss: 3.0040, Val Loss: 3.9875\n",
            "Epoch 12/60, Train Loss: 2.8124, Val Loss: 4.0029\n",
            "Epoch 13/60, Train Loss: 2.6222, Val Loss: 4.0128\n",
            "Epoch 14/60, Train Loss: 2.4361, Val Loss: 4.0290\n",
            "Early stopping\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg_scores = evaluate_model(\n",
        "    model, val_loader, tokenizer, max_len=MAX_TGT_LEN,\n",
        "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        ")\n",
        "print(avg_scores)\n"
      ],
      "metadata": {
        "id": "RPB78sa0T9Qa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "393da040-2aaa-48f3-fd6a-db91f2de1be9"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge1': 0.306224923270959, 'rouge2': 0.08354588017017366, 'rougeL': 0.24818029458176016}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "checkpoint_path = \"/content/best_transformer_model.pth\"\n",
        "assert os.path.exists(checkpoint_path), \"Checkpoint file does not exist!\"\n",
        "\n",
        "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
        "state_dict = torch.load(checkpoint_path, map_location=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_Yn2szHfwjx",
        "outputId": "190ab05a-2ccb-4e31-dd57-9cc61f367154"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-40-4fd1fc5e96dd>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(checkpoint_path, map_location=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder_embedding): Embedding(30522, 512)\n",
              "  (decoder_embedding): Embedding(30522, 512)\n",
              "  (positional_encoding): PositionalEncoding()\n",
              "  (encoder_layers): ModuleList(\n",
              "    (0-5): 6 x EncoderLayer(\n",
              "      (self_attn): MultiHeadAttention(\n",
              "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (feed_forward): PositionWiseFeedForward(\n",
              "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (decoder_layers): ModuleList(\n",
              "    (0-5): 6 x DecoderLayer(\n",
              "      (self_attn): MultiHeadAttention(\n",
              "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (cross_attn): MultiHeadAttention(\n",
              "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (feed_forward): PositionWiseFeedForward(\n",
              "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (fc): Linear(in_features=512, out_features=30522, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(model, input_text, tokenizer, max_input_len, max_output_len, device):\n",
        "\n",
        "    input_encoding = tokenizer(\n",
        "        input_text,\n",
        "        max_length=max_input_len,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    input_ids = input_encoding['input_ids']\n",
        "\n",
        "    src_mask = (input_ids != tokenizer.pad_token_id).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        src_embedded = model.dropout(model.positional_encoding(model.encoder_embedding(input_ids)))\n",
        "        enc_output = src_embedded\n",
        "        for enc_layer in model.encoder_layers:\n",
        "            enc_output = enc_layer(enc_output, src_mask)\n",
        "\n",
        "    tgt_tokens = torch.tensor([[tokenizer.cls_token_id]], device=device)\n",
        "\n",
        "    for _ in range(max_output_len):\n",
        "        tgt_mask = torch.tril(torch.ones((tgt_tokens.size(1), tgt_tokens.size(1)), device=device)).bool().unsqueeze(0)\n",
        "        tgt_embedded = model.dropout(model.positional_encoding(model.decoder_embedding(tgt_tokens)))\n",
        "        dec_output = tgt_embedded\n",
        "\n",
        "        for dec_layer in model.decoder_layers:\n",
        "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
        "\n",
        "        predictions = model.fc(dec_output[:, -1, :])\n",
        "        next_token = predictions.argmax(dim=-1).unsqueeze(0)\n",
        "\n",
        "        if next_token.item() == tokenizer.sep_token_id:\n",
        "            break\n",
        "\n",
        "        tgt_tokens = torch.cat([tgt_tokens, next_token], dim=1)\n",
        "\n",
        "    summary = tokenizer.decode(tgt_tokens.squeeze().tolist(), skip_special_tokens=True)\n",
        "\n",
        "    return summary\n"
      ],
      "metadata": {
        "id": "MJp260iHrsU1"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input text\n",
        "input_text = \"\"\"Hannah: Hey, do you have Betty's number?\n",
        "Amanda: Lemme check\n",
        "Hannah: <file_gif>\n",
        "Amanda: Sorry, can't find it.\n",
        "Amanda: Ask Larry\n",
        "Amanda: He called her last time we were at the park together\n",
        "Hannah: I don't know him well\n",
        "Hannah: <file_gif>\n",
        "Amanda: Don't be shy, he's very nice\n",
        "Hannah: If you say so..\n",
        "Hannah: I'd rather you texted him\n",
        "Amanda: Just text him ðŸ™‚\n",
        "Hannah: Urgh.. Alright\n",
        "Hannah: Bye\n",
        "Amanda: Bye bye\"\"\"\n",
        "\n",
        "# Generate summary\n",
        "summary = inference(model=model,input_text=input_text,tokenizer=tokenizer,max_input_len= 500, max_output_len= 100,device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "print(\"Generated Summary:\", summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fz5QXBnrtDs",
        "outputId": "f1700c37-5fb3-42b1-c19c-10bd134995ca"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Summary: amanda is looking for her mother. amanda is not sure if she can't help her.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generated Summary : amanda is looking for her mother. amanda is not sure if she can't help her.\n",
        "\n",
        "Actual Summary: Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry."
      ],
      "metadata": {
        "id": "6UyoGnqzvQXP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. **Loss**:\n",
        "   - **Training Loss**: The training loss steadily decreases throughout the epochs, showing effective learning by the model.\n",
        "   - **Validation Loss**: The validation loss decreases consistently until Epoch 11, after which it plateaus and slightly increases. Early stopping was appropriately used to prevent overfitting.\n",
        "\n",
        "### 2. **ROUGE Scores**:\n",
        "   - **ROUGE-1**: 0.3062\n",
        "   - **ROUGE-2**: 0.0835\n",
        "   - **ROUGE-L**: 0.2482\n",
        "   These scores show a slight improvement over the earlier results, indicating better capture of key unigrams (ROUGE-1) and structural coherence (ROUGE-L). However, the low ROUGE-2 score suggests that capturing consecutive bigrams is still challenging.\n",
        "\n",
        "### 3. **Evaluation Observations**:\n",
        "   - **Relevance**: The model demonstrates a moderate ability to generate relevant summaries, as reflected by the improved ROUGE-1 and ROUGE-L scores.\n",
        "   - **Coherence**: The logical structure and clarity of the summaries have marginally improved, but the summaries still might miss some detailed connections (low ROUGE-2).\n",
        "   - **Conciseness**: The summaries remain concise and capture essential content, but some key details may still be missing.\n",
        "\n",
        "These results indicate progress in the model's ability to generate summaries, with better recall and structural alignment compared to earlier attempts. Let me know if you need further details or insights!"
      ],
      "metadata": {
        "id": "eyvravkCuzTX"
      }
    }
  ]
}